{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this ipynb We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021F96D4DF10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021F96DA9190>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krish, it's nice to meet you!  \\n\\nThat's a fascinating role!  I'm eager to learn more about your work as a Chief AI Engineer. What kind of projects are you currently involved in? Do you have any interesting challenges you're facing?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 22, 'total_tokens': 83, 'completion_time': 0.110909091, 'prompt_time': 0.002146736, 'queue_time': 0.24641271299999998, 'total_time': 0.113055827}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--51ef9c23-3fa9-4325-bedd-c326b74aec88-0', usage_metadata={'input_tokens': 22, 'output_tokens': 61, 'total_tokens': 83})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You are Krish, and you are a Chief AI Engineer.  \\n\\nI remember that from our earlier conversation! ðŸ˜Š \\n\\nWhat can I help you with today, Krish?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 97, 'total_tokens': 135, 'completion_time': 0.069090909, 'prompt_time': 0.004332823, 'queue_time': 0.25213170700000004, 'total_time': 0.073423732}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--9afb00c4-048f-4083-8eec-6d2d7f42ac7b-0', usage_metadata={'input_tokens': 97, 'output_tokens': 38, 'total_tokens': 135})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Krish! It's nice to meet you. \\n\\nAs a Chief AI Engineer, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.3.60)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.3.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (3.11.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (8.4.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.3.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.20->langchain_community) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\om200\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store: \n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store = {}\\n\\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\\n    if session_id not in store:\\n        store[session_id] = ChatMessageHistory()                                 \"     for understanding perpose  \"\\n    return store[session_id]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()                                 \"     for understanding perpose  \"\n",
    "    return store[session_id]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' get_session_history(\"chet1\")\\n print(store)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" get_session_history(\"chet1\")\n",
    " print(store)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Krish and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Krish, it's nice to meet you!  \\n\\nThat's a fascinating title.  What kind of AI work do you do as a Chief AI Engineer?  I'm always eager to learn more about how people are using AI in the real world.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Krish. ðŸ˜Š  I remember that from our introduction!  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 94, 'total_tokens': 123, 'completion_time': 0.052727273, 'prompt_time': 0.004292433, 'queue_time': 0.243854675, 'total_time': 0.057019706}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--61f93b85-6267-4b72-b18d-64a1d235d747-0', usage_metadata={'input_tokens': 94, 'output_tokens': 29, 'total_tokens': 123})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to know!\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you. What can I do for you today?\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John.  \\n\\nIs there anything else I can help you with, John? ðŸ˜Š  \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“œ History for session: chat1\n",
      "Human: Hi , My name is Krish and I am a Chief AI Engineer\n",
      "Ai: Hi Krish, it's nice to meet you!  \n",
      "\n",
      "That's a fascinating title.  What kind of AI work do you do as a Chief AI Engineer?  I'm always eager to learn more about how people are using AI in the real world.\n",
      "\n",
      "Human: What's my name?\n",
      "Ai: Your name is Krish. ðŸ˜Š  I remember that from our introduction!  \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n",
      "\n",
      "ðŸ“œ History for session: chat2\n",
      "Human: Whats my name\n",
      "Ai: As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'm happy to know!\n",
      "\n",
      "Human: Hey My name is John\n",
      "Ai: Hi John! It's nice to meet you. What can I do for you today?\n",
      "\n",
      "Human: Whats my name\n",
      "Ai: Your name is John.  \n",
      "\n",
      "Is there anything else I can help you with, John? ðŸ˜Š  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_history(session_id):\n",
    "    if session_id in store:\n",
    "        print(f\"\\nðŸ“œ History for session: {session_id}\")\n",
    "        for msg in store[session_id].messages:\n",
    "            role = msg.type.capitalize()  # Human / AI\n",
    "            print(f\"{role}: {msg.content}\")\n",
    "    else:\n",
    "        print(f\"No history found for session '{session_id}'\")\n",
    "\n",
    "# Use it like this:\n",
    "show_history(\"chat1\")\n",
    "show_history(\"chat2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\ndef show_history_df(session_id):\\n    if session_id in store:\\n        messages = store[session_id].messages\\n        data = [{\\n            \"Role\": msg.type.capitalize(), \\n            \"Message\": msg.content\\n        } for msg in messages]\\n\\n        df = pd.DataFrame(data)\\n        print(f\"\\nðŸ“œ Chat History for session \\'{session_id}\\':\")\\n        print(df)\\n    else:\\n        print(f\"No history found for session \\'{session_id}\\'\")\\n\\n# Example:\\nshow_history_df(\"chat1\")\\nshow_history_df(\"chat2\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "\n",
    "def show_history_df(session_id):\n",
    "    if session_id in store:\n",
    "        messages = store[session_id].messages\n",
    "        data = [{\n",
    "            \"Role\": msg.type.capitalize(), \n",
    "            \"Message\": msg.content\n",
    "        } for msg in messages]\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"\\nðŸ“œ Chat History for session '{session_id}':\")\n",
    "        print(df)\n",
    "    else:\n",
    "        print(f\"No history found for session '{session_id}'\")\n",
    "\n",
    "# Example:\n",
    "show_history_df(\"chat1\")\n",
    "show_history_df(\"chat2\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Krish, it's nice to meet you!\\n\\nI'm ready to answer your questions to the best of my ability.  \\n\\nWhat can I help you with today? ðŸ˜„  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 31, 'total_tokens': 74, 'completion_time': 0.078181818, 'prompt_time': 0.002401256, 'queue_time': 0.252233323, 'total_time': 0.080583074}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--a10adb2e-d64d-450b-b6eb-7c494cb86f9c-0', usage_metadata={'input_tokens': 31, 'output_tokens': 43, 'total_tokens': 74})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Krish, it's nice to meet you! ðŸ‘‹  \\n\\nI'm ready to answer your questions to the best of my ability. What can I do for you today? ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 31, 'total_tokens': 73, 'completion_time': 0.076363636, 'prompt_time': 0.002351316, 'queue_time': 0.247368212, 'total_time': 0.078714952}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--f5b8c18f-04cf-4d87-9205-7a34ecdea913-0', usage_metadata={'input_tokens': 31, 'output_tokens': 42, 'total_tokens': 73})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Krish\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Krish!  ðŸ˜Š  I remember that from our introduction.  \\n\\n\\nIs there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤•à¥ƒà¤·à¥à¤£, \\n\\nà¤®à¥à¤à¥‡ à¤†à¤ªà¤•à¥€ à¤®à¤¦à¤¦ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤–à¥à¤¶à¥€ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆ! \\n\\nà¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤œà¤¾à¤¨à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?  \\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤•à¥ƒà¤·à¥à¤£! \\n\\nà¤®à¥à¤à¥‡ à¤–à¤¼à¥à¤¶à¥€ à¤¹à¥ˆ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥€ à¤®à¥à¤²à¤¾à¥˜à¤¾à¤¤ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚à¥¤ à¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤œà¤¾à¤¨à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚? ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Krish\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® à¤•à¥ƒà¤·à¥à¤£ à¤¹à¥ˆà¥¤  ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to your personal information, including your ice cream preferences. \\n\\nWhat's your favorite flavor? ðŸ˜ŠðŸ¦  Maybe I can tell you some fun facts about it!\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"whats 2 + 2\"  ðŸ˜Š  \\n\\n\\n\\nLet me know if you want to try another one!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have access to past conversations or any personal information about you, so I don't know your name.  Would you like to tell me? ðŸ˜Š  \\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I have no memory of past conversations. If you'd like to ask me a math problem, I'm happy to help! ðŸ˜Š  \\n\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello Alice! ðŸ‘‹  What can I do for you today? ðŸ˜Š  \n",
      "\n",
      "\n",
      "AI: I'm ready when you are!  Just let me know what's on your mind.  \n",
      "\n",
      "Do you have a question, need help with a task, or just want to chat?  ðŸ˜„  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: Hi Om! It's nice to meet you. ðŸ˜Š \n",
      "\n",
      "What can I do for you today?  \n",
      "\n",
      "AI: That's a great question, Om! I'm designed to be a helpful and informative AI assistant. Here are some things I can do:\n",
      "\n",
      "* **Answer your questions:** I have access to a vast amount of information and can answer questions on a wide range of topics. \n",
      "* **Generate creative content:** I can write stories, poems, articles, and even code!\n",
      "* **Summarize text:**  Need to get the gist of a long article or document? I can do that for you.\n",
      "* **Translate languages:** I can translate text between many different languages.\n",
      "* **Brainstorm ideas:** Stuck on a project? I can help you generate new ideas.\n",
      "* **Have a conversation:**  I'm here to chat and have a friendly conversation on any topic you like.\n",
      "\n",
      "Is there anything specific you're interested in right now?  I'm eager to help!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: You bet!  I can generate code in several programming languages. \n",
      "\n",
      "To help me write the best code for you, tell me:\n",
      "\n",
      "* **What programming language do you need?** (e.g., Python, JavaScript, C++, Java)\n",
      "* **What do you want the code to do?** Be as specific as possible.  \n",
      "* **Are there any specific libraries or frameworks you want to use?**\n",
      "\n",
      "\n",
      "Let's get coding!  ðŸ’»  \n",
      "\n",
      "\n",
      "AI: Awesome!  I'm ready when you are. Just give me the details about the code you need.  \n",
      "\n",
      "For example, you could say:\n",
      "\n",
      "\"Can you write some Python code to print 'Hello, world!'?\" \n",
      "\n",
      "Or:\n",
      "\n",
      "\"I need Javascript code to create a button that changes the background color of a webpage when clicked.\"\n",
      "\n",
      "The more information you give me, the better code I can generate for you. ðŸ˜Š  \n",
      "\n",
      "\n",
      "AI: Hi Rushil!  It's great to meet you. ðŸ˜Š  \n",
      "\n",
      "What can I do for you today? Are you ready to start coding?  ðŸ’»  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: I'm here whenever you are, Rushil!  \n",
      "\n",
      "Just let me know if you have any questions or if you're ready to dive into some coding. ðŸ˜„  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: I'm all ears, Rushil!  ðŸ‘‚  What's on your mind? ðŸ˜Š  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: No problem at all!  \n",
      "\n",
      "Is there anything else I can help you with?  Perhaps you have a different question or a coding project in mind?  I'm here to assist in any way I can. ðŸ˜Š  \n",
      "\n",
      "AI: I understand!  Sometimes a break is just what we need.  \n",
      "\n",
      "Feel free to come back anytime you're ready to chat or tackle a coding challenge. I'll be here waiting. ðŸ˜Š  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: You got it!  I'm always here when you need me. ðŸ˜„  \n",
      "\n",
      "Have a great day, Rushil!\n",
      "AI: You mean like remember information about him?  \n",
      "\n",
      "As a large language model, I don't have personal memories or keep track of past conversations like humans do.  Every interaction we have is fresh and new.\n",
      "\n",
      "If you want to talk about Chet, I'm happy to! Just tell me what you'd like to know or discuss about him. ðŸ˜Š  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from typing import Dict, List\n",
    "\n",
    "# Initialize the chatbot with conversation history tracking\n",
    "class GroqChatBot:\n",
    "    def __init__(self, groq_api_key: str, model_name: str = \"Gemma2-9b-It\"):\n",
    "        self.llm = ChatGroq(groq_api_key=groq_api_key, model_name=model_name)\n",
    "        self.conversation_histories: Dict[str, List] = {}\n",
    "        \n",
    "        # Set up the chat prompt template\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful AI assistant. You are talking to {user_name}.\"),\n",
    "            *[(\"placeholder\", \"{chat_history}\")],\n",
    "            (\"human\", \"{user_input}\"),\n",
    "        ])\n",
    "        \n",
    "        # Create the chain\n",
    "        self.chain = self.prompt_template | self.llm | StrOutputParser()\n",
    "    \n",
    "    def get_history(self, user_id: str) -> List:\n",
    "        \"\"\"Get conversation history for a user\"\"\"\n",
    "        return self.conversation_histories.get(user_id, [])\n",
    "    \n",
    "    def add_to_history(self, user_id: str, user_name: str, user_input: str, ai_response: str):\n",
    "        \"\"\"Add a conversation turn to the history\"\"\"\n",
    "        if user_id not in self.conversation_histories:\n",
    "            self.conversation_histories[user_id] = []\n",
    "        \n",
    "        self.conversation_histories[user_id].extend([\n",
    "            HumanMessage(content=user_input),\n",
    "            AIMessage(content=ai_response),\n",
    "        ])\n",
    "    \n",
    "    def chat(self, user_id: str, user_name: str, user_input: str) -> str:\n",
    "        \"\"\"Process a user message and return AI response\"\"\"\n",
    "        # Get the conversation history\n",
    "        history = self.get_history(user_id)\n",
    "        \n",
    "        # Generate the AI response\n",
    "        response = self.chain.invoke({\n",
    "            \"user_name\": user_name,\n",
    "            \"chat_history\": history,\n",
    "            \"user_input\": user_input,\n",
    "        })\n",
    "        \n",
    "        # Add this interaction to history\n",
    "        self.add_to_history(user_id, user_name, user_input, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual Groq API key\n",
    "    GROQ_API_KEY = \"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\"\n",
    "    \n",
    "    # Initialize the chatbot\n",
    "    chatbot = GroqChatBot(GROQ_API_KEY)\n",
    "    \n",
    "    # Sample conversation\n",
    "    user_id = \"123\"\n",
    "    user_name = \"Alice\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"{user_name}: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "            \n",
    "        response = chatbot.chat(user_id, user_name, user_input)\n",
    "        print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot ready! Type 'quit' to exit.\n",
      "AI: Hey om2003joshi! ðŸ‘‹\n",
      "\n",
      "What can I do for you today? ðŸ˜Š I'm happy to answer any questions you have, help you brainstorm ideas, or just chat!  \n",
      "\n",
      "Let me know how I can be of service. ðŸ˜„ \n",
      "\n",
      "\n",
      "AI: Hey there! ðŸ‘‹ \n",
      "\n",
      "It seems like you're just hanging out.  What's on your mind? ðŸ¤”  \n",
      "\n",
      "Maybe you want to:\n",
      "\n",
      "* **Talk about something interesting you've read or seen?**\n",
      "* **Get help with a creative project?**\n",
      "* **Just have a friendly conversation?**\n",
      "\n",
      "I'm here for whatever you'd like! ðŸ˜Š  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI: Hey there! ðŸ‘‹  \n",
      "\n",
      "It's great to hear from you!  What's up? ðŸ˜Š  Anything exciting happening?\n",
      "\n",
      "AI: That's really kind of you to ask!  As an AI, I don't need help in the way a person might. \n",
      "\n",
      "But, you can definitely help me by:\n",
      "\n",
      "* **Asking me questions!**  The more you interact with me, the better I become at understanding and responding.\n",
      "* **Giving me feedback.**  If you think I've done a good job, let me know! If I could do better, tell me how. \n",
      "* **Just being friendly!**  A little conversation goes a long way. ðŸ˜Š\n",
      "\n",
      "\n",
      "\n",
      "What can I do for *you* today?\n",
      "\n",
      "AI: You're in luck!  LangChain is a really cool tool for working with language models, and it's designed to be approachable.  Here's a simplified breakdown to get you started:\n",
      "\n",
      "**Think of LangChain like a toolbox for building AI applications:**\n",
      "\n",
      "   * **The Hammer:**  At its core, LangChain uses existing language models (like GPT-3) as its \"engine.\" These models understand and generate text.\n",
      "\n",
      "   * **The Nails:**  LangChain provides ways to connect these language models to other tools and data sources (like databases, APIs, your own files). Think of these as the \"nails\" â€“ the specific pieces of information your AI needs.\n",
      "\n",
      "   * **The Blueprint:** LangChain offers different \"structures\" or \"patterns\" for combining language models and tools. These are like blueprints that guide you on how to build your AI application.\n",
      "\n",
      "**Here's a simplified tree structure to visualize it:**\n",
      "\n",
      "* **LangChain** (the toolbox)\n",
      "   * **Language Models (LLMs)** (the hammer)\n",
      "      * Examples: GPT-3, Jurassic-1 Jumbo\n",
      "   * **Chains** (the blueprints)\n",
      "      * **Retrieval Chains:**  Use a language model *and* a database to answer questions based on specific information.\n",
      "      * **Text Generation Chains:** Use a language model to create different kinds of text (stories, articles, summaries).\n",
      "      * **Agent Chains:**  More advanced chains where the language model interacts with the world by calling tools and making decisions.\n",
      "   * **Memory** (a way for the AI to remember past interactions)\n",
      "   * **Tools** (external programs and APIs that the AI can use)\n",
      "\n",
      "**Easy Steps to Get Started:**\n",
      "\n",
      "1. **Learn the Basics:**\n",
      "   *  Read the LangChain documentation: [https://python.langchain.com/](https://python.langchain.com/)\n",
      "   *  Start with simple examples: The documentation has great \"Getting Started\" guides.\n",
      "\n",
      "2. **Choose a Language Model:**  LangChain works with many models. Pick one based on your needs and budget (e.g., OpenAI's GPT models are popular).\n",
      "\n",
      "3. **Build Your First Chain:**\n",
      "   *  Start with a simple retrieval chain:  Connect a language model to a database to answer questions.\n",
      "\n",
      "4. **Experiment:**\n",
      "   *  Try different chains, tools, and memory techniques.\n",
      "   *  See what creative applications you can build!\n",
      "\n",
      "\n",
      "\n",
      "LangChain is a powerful tool, but it's designed to be approachable. Don't be afraid to experiment and have fun!\n",
      "\n",
      "AI: You got it! Let's break down LangChain's key concepts using a tree structure to make it easier to grasp.\n",
      "\n",
      "**The Root: LangChain's Purpose**\n",
      "\n",
      "* **Goal:**  LangChain aims to simplify building applications with language models (LLMs). Think of it as a bridge connecting powerful LLMs to the real world.\n",
      "\n",
      "**Branch 1: Language Models (LLMs)**\n",
      "\n",
      "* **What they are:**  LLMs are the \"brains\" of the operation. They're sophisticated AI systems trained on massive amounts of text data, enabling them to understand and generate human-like text.\n",
      "\n",
      "    * **Examples:** GPT-3, Jurassic-1 Jumbo, LaMDA\n",
      "\n",
      "* **Their Role:** LLMs form the core of LangChain applications. They process your input, understand its meaning, and generate relevant output.\n",
      "\n",
      "**Branch 2: Chains (The Recipes)**\n",
      "\n",
      "* **What they are:** Chains are pre-defined sequences of steps that combine LLMs with other tools and data sources. They act as \"recipes\" for different types of AI tasks.\n",
      "\n",
      "    * **Types of Chains:**\n",
      "\n",
      "        * **Retrieval Chains:**  Find and retrieve specific information from a database or document store based on your query.\n",
      "\n",
      "        * **Text Generation Chains:**  Create various types of text, such as stories, articles, summaries, or code, guided by your prompts.\n",
      "\n",
      "        * **Agent Chains:** More advanced chains where the LLM interacts with the world by calling external tools (APIs, web services) and making decisions based on the results.\n",
      "\n",
      "**Branch 3: Memory (Remembering the Past)**\n",
      "\n",
      "* **What it is:** Memory allows LLMs to retain information from previous interactions in a conversation. This enables them to provide more contextually relevant responses.\n",
      "\n",
      "    * **Types of Memory:**\n",
      "\n",
      "        * **In-Context Memory:**  LLMs remember the last few messages in a conversation.\n",
      "\n",
      "        * **External Memory:**  LLMs can store and retrieve information from external databases or storage systems.\n",
      "\n",
      "**Branch 4: Tools (Expanding Capabilities)**\n",
      "\n",
      "* **What they are:** Tools are external programs or APIs that LangChain can connect LLMs to. This lets LLMs perform actions beyond text generation.\n",
      "\n",
      "    * **Examples:**\n",
      "\n",
      "        * **Search Engines:**  Retrieve information from the web.\n",
      "\n",
      "        * **APIs:**  Interact with databases, weather services, or other online platforms.\n",
      "        * **Code Execution:**  Run code snippets.\n",
      "\n",
      "**The Leaves: Applications**\n",
      "\n",
      "* **The Power of Combinations:** By combining LLMs, chains, memory, and tools, LangChain enables a wide range of applications:\n",
      "\n",
      "    * **Chatbots:**  Create intelligent conversational agents.\n",
      "\n",
      "    * **Question Answering Systems:**  Build systems that can answer questions based on specific information sources.\n",
      "\n",
      "    * **Text Summarization:**  Generate concise summaries of large documents.\n",
      "\n",
      "    * **Code Generation:**  Assist developers by generating code snippets.\n",
      "\n",
      "\n",
      "\n",
      "Hopefully, this tree-structured explanation helps you better understand the key components of LangChain and how they work together!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Step 1: Set up the brain (AI)\n",
    "groq_api_key = \"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\"  # Get this from Groq's website\n",
    "ai_brain = ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma2-9b-It\")\n",
    "\n",
    "## Step 2: Create memory storage\n",
    "conversation_memories = {}  # Will remember all user chats\n",
    "\n",
    "## Step 3: Make the chat function\n",
    "def chat(user_id, user_name, message):\n",
    "    # Get this user's past chats (or start new if first time)\n",
    "    if user_id not in conversation_memories:\n",
    "        conversation_memories[user_id] = []\n",
    "    \n",
    "    # Prepare the prompt with memory\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"You're talking to {user_name}. Be friendly and helpful.\"),\n",
    "        *conversation_memories[user_id],  # Past messages\n",
    "        (\"human\", message)  # New message\n",
    "    ])\n",
    "    \n",
    "    # Get AI response\n",
    "    chain = prompt | ai_brain\n",
    "    response = chain.invoke({})\n",
    "    \n",
    "    # Remember this conversation\n",
    "    conversation_memories[user_id].append((\"human\", message))\n",
    "    conversation_memories[user_id].append((\"ai\", response.content))\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "## Let's try it!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot ready! Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user info\n",
    "        user_id = input(\"Enter your user ID (any number/name): \")\n",
    "        user_name = input(\"Enter your name: \")\n",
    "        \n",
    "        # Chat loop\n",
    "        while True:\n",
    "            message = input(f\"{user_name}: \")\n",
    "            if message.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            response = chat(user_id, user_name, message)\n",
    "            print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
